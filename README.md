# EgoDrive: An Egocentric, Multimodal Dataset and Methods for Driver Behavior Analysis

EgoDrive: Multimodal Driver Behavior Recognition using Project Aria

## ğŸš— Overview

EgoDrive is the first egocentric, multimodal dataset for driver behavior analysis captured using Meta's Project Aria glasses. This proof-of-concept work demonstrates the technical feasibility of real-time driver behavior recognition through multimodal sensor fusion, combining RGB video, eye gaze tracking, hand pose estimation, IMU data, and semantic object detection.

**Key Features:**

- ğŸ¥ **138,024 frames** of synchronized multimodal data (2.5 hours)
- ğŸ‘ï¸ **Six behavioral classes**: Driving, Left/Right/Rear Mirror Check, Mobile Phone Usage, Idle
- âš¡ **Real-time performance**: Sub-3ms inference with 97.4% accuracy
- ğŸ”¬ **Proof-of-concept validation** for multimodal egocentric driver monitoring
- ğŸ—ï¸ **Two model variants**: EgoDriveMax (accuracy-focused) and EgoDriveRT (efficiency-focused)

## ğŸ” Technical Contributions

### Multimodal Architecture

- **Transformer-based fusion** of heterogeneous sensor streams
- **Modality-specific encoders** for RGB, gaze, hands, IMU, and object detection
- **Temporal alignment strategies** for synchronized multimodal processing

### Efficiency Breakthrough

- **400x parameter reduction** (42M â†’ 104K parameters)
- **Minimal accuracy loss** (98.6% â†’ 97.4%)
- **Real-time inference** (1595ms â†’ 2.65ms on Apple M4)

### Dataset Methodology

- **Synchronized data collection** using Project Aria glasses
- **Temporal alignment framework** for heterogeneous sensor streams
- **Annotation protocols** for behavioral class identification

## ğŸ¤– Model Architecture

We developed two variants of our Multimodal Transformer architecture:

| Model       | Blocks | Heads | Feature Dim | RGB | Params | Inference Time | Accuracy |
| ----------- | ------ | ----- | ----------- | --- | ------ | -------------- | -------- |
| EgoDriveMax | 2      | 4     | 256         | âœ“   | 42M    | 1595ms         | 98.6%    |
| EgoDriveRT  | 1      | 2     | 32          | âœ—   | 104K   | 2.65ms         | 97.4%    |

### Architecture Components:

- **RGB Encoder**: Swin-Tiny + ResNet-18 motion stream
- **Gaze Encoder**: Linear projection + 1D convolution
- **Hands Encoder**: Missing value handling + temporal attention
- **Object Detection Encoder**: YOLO v11 features + temporal modeling
- **IMU Encoder**: Stacked 1D CNNs + GRU for dynamics

## ğŸš€ Getting Started

### Prerequisites

```bash
git clone https://github.com/your-username/egodrive.git
cd egodrive
conda create -n egodrive python=3.10
conda activate egodrive
pip install -r requirements.txt
```

**Dependencies**

- PyTorch >= 1.12.0
- Transformers
- OpenCV
- NumPy
- Project Aria Tools (for data processing)

### ğŸ“¦ Dataset

**Data Structure** The EgoDrive dataset contains synchronized streams from Project Aria glasses:

```
egodrive_dataset/
â”œâ”€â”€ rgb_videos/          # 15fps RGB camera
â”œâ”€â”€ gaze_data/           # 30fps eye tracking
â”œâ”€â”€ hand_poses/          # 3D hand landmarks
â”œâ”€â”€ imu_data/            # 800Hz-1KHz inertial data
â”œâ”€â”€ object_detections/   # YOLO v11 in-cabin detection
â””â”€â”€ annotations/         # Behavioral class labels
```

**Behavioral Classes**

- Driving - Normal forward driving attention
- Left Mirror Check - Checking left side mirror
- Right Mirror Check - Checking right side mirror
- Rear-view Mirror Check - Checking rear-view mirror
- Mobile Phone Usage - Interacting with mobile device
- Idle - Stationary/waiting periods

**Data Characteristics**

- Temporal Resolution: 32-frame sequences (2 seconds)
- Sampling Rates: RGB (15fps), Gaze (30fps), IMU (800Hz-1KHz)
- Annotation: Frame-by-frame behavioral labels
- Scale: Proof-of-concept single-participant dataset

## ğŸƒâ€â™‚ï¸ Training and Inference

### Training

```bash
python train.py --config configs/egodrive_max.yaml
python train.py --config configs/egodrive_rt.yaml
```

### Inference

```bash
python inference.py --model_path checkpoints/egodrive_rt.pth --input_dir data/test/
```

### Evaluation

```bash
python evaluate.py --model_path checkpoints/egodrive_rt.pth --test_data data/test/
```

## ğŸ“Š Results

### Overall Performance

| Metric         | EgoDriveMax | EgoDriveRT |
| -------------- | ----------- | ---------- |
| Accuracy       | 98.6%       | 97.4%      |
| F1-Score       | 98.0%       | 96.6%      |
| Parameters     | 42M         | 104K       |
| Inference Time | 1595ms      | 2.65ms     |

### Per-Action Results

| Action       | EgoDriveMax Acc | EgoDriveRT Acc |
| ------------ | --------------- | -------------- |
| Driving      | 99.3%           | 98.7%          |
| Left Mirror  | 96.9%           | 100%           |
| Right Mirror | 97.4%           | 94.9%          |
| Rear Mirror  | 97.9%           | 91.2%          |
| Mobile Phone | 94.1%           | 96.3%          |
| Idle         | 100%            | 97.1%          |

## âš ï¸ Limitations and Scope

This work presents a proof-of-concept study with the following limitations:

- **Single Participant**: Dataset collected from one driver in controlled conditions
- **Limited Generalizability**: Results may not extend to diverse populations
- **Controlled Environment**: Laboratory-style data collection setup
- **Scope**: Technical feasibility demonstration rather than production system

**Future Work**: Multi-participant validation, real-world testing, and privacy-preserving deployment strategies.

## ğŸ“„ Citation

If you use EgoDrive in your research, please cite:

```bibtex
@inproceedings{egodrive2023,
    title={EgoDrive: An Egocentric, Multimodal Dataset and Methods for Driver Behavior Analysis},
    author={Anonymous Authors},
    booktitle={Proceedings of RANLP 2023},
    year={2023}
}
```

## ğŸ”’ Ethics and Privacy

- **Data Collection**: Conducted with informed consent and GDPR compliance
- **Privacy**: Designed for on-device processing to keep PII local
- **Safety**: Non-intrusive data collection ensuring driver safety
- **Future Deployment**: Privacy-preserving principles for real-world applications

## ğŸ“ License

This dataset and code are released under the Creative Commons Attribution-NonCommercial 4.0 International License (CC BY-NC 4.0).

> Note: This is a research dataset intended for academic and non-commercial use only.

## ğŸ¤ Contributing

We welcome contributions! Please see `CONTRIBUTING.md` for guidelines.

## ğŸ“ Contact

For questions about the dataset or technical issues, please open an issue or contact [[your-email@institution.edu](mailto\:your-email@institution.edu)].

## âš¡ Key Insight

This work demonstrates that effective multimodal driver behavior recognition is technically feasible with dramatic efficiency improvements, paving the way for practical egocentric driver monitoring systems.

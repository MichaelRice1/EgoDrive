{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing keys for loading model: []\n",
      "Unexpected keys for loading model: []\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "sys.path.append('./perception_models')\n",
    "import core.vision_encoder.pe as pe\n",
    "import core.vision_encoder.transforms as transforms\n",
    "from huggingface_hub import hf_hub_download\n",
    "import cv2\n",
    "\n",
    "\n",
    "def preprocess_video(video_path, num_frames=30, transform=None, return_first_frame_for_demo=True):\n",
    "    print(video_path)\n",
    "    cap = cv2.VideoCapture(video_path, apiPreference=cv2.CAP_ANY)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    frame_indices = [int(i * (total_frames / num_frames)) for i in range(num_frames)]\n",
    "    preprocessed_frames = []\n",
    "    first_frame = None\n",
    "\n",
    "    current_index = 0\n",
    "    grabbed_index = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        if current_index == frame_indices[grabbed_index]:\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            pil_img = Image.fromarray(frame_rgb)\n",
    "            if transform:\n",
    "                frame_tensor = transform(pil_img)\n",
    "            else:\n",
    "                frame_tensor = transforms.ToTensor()(pil_img)\n",
    "\n",
    "            preprocessed_frames.append(frame_tensor)\n",
    "\n",
    "            if grabbed_index == 0 and return_first_frame_for_demo:\n",
    "                first_frame = frame_rgb\n",
    "\n",
    "            grabbed_index += 1\n",
    "            if grabbed_index >= len(frame_indices):\n",
    "                break\n",
    "\n",
    "        current_index += 1\n",
    "\n",
    "    cap.release()\n",
    "    return torch.stack(preprocessed_frames, dim=0), first_frame\n",
    "\n",
    "local_ckpt_path = hf_hub_download(\n",
    "    repo_id=\"facebook/PE-Core-B16-224\",\n",
    "    filename=\"PE-Core-B16-224.pt\"\n",
    ")\n",
    "device = torch.device(\"mps\")\n",
    "model_name = 'PE-Core-B16-224'\n",
    "\n",
    "model = pe.CLIP.from_config(model_name, pretrained=True, checkpoint_path=local_ckpt_path)\n",
    "model = model.to(device)\n",
    "\n",
    "preprocess = transforms.get_image_transform(model.image_size)\n",
    "tokenizer = transforms.get_text_tokenizer(model.context_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T_destination', '__annotations__', '__call__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__setstate__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_apply', '_backward_hooks', '_backward_pre_hooks', '_buffers', '_call_impl', '_compiled_call_impl', '_forward_hooks', '_forward_hooks_always_called', '_forward_hooks_with_kwargs', '_forward_pre_hooks', '_forward_pre_hooks_with_kwargs', '_get_backward_hooks', '_get_backward_pre_hooks', '_get_name', '_is_full_backward_hook', '_load_from_state_dict', '_load_state_dict_post_hooks', '_load_state_dict_pre_hooks', '_maybe_warn_non_full_backward_hook', '_modules', '_named_members', '_non_persistent_buffers_set', '_parameters', '_register_load_state_dict_pre_hook', '_register_state_dict_hook', '_replicate_for_data_parallel', '_save_to_state_dict', '_slow_forward', '_state_dict_hooks', '_state_dict_pre_hooks', '_version', '_wrapped_call_impl', 'add_module', 'apply', 'attn_mask', 'attn_pool', 'available_configs', 'bfloat16', 'buffers', 'build_causal_mask', 'build_cls_mask', 'call_super_init', 'children', 'compile', 'context_length', 'cpu', 'cuda', 'double', 'dump_patches', 'encode_image', 'encode_text', 'encode_video', 'eval', 'extra_repr', 'float', 'forward', 'from_config', 'get_buffer', 'get_extra_state', 'get_parameter', 'get_submodule', 'half', 'heads', 'image_size', 'ipu', 'layers', 'ln_final', 'load_ckpt', 'load_state_dict', 'logit_scale', 'modules', 'mtia', 'named_buffers', 'named_children', 'named_modules', 'named_parameters', 'num_pos', 'output_dim', 'output_tokens', 'pad_id', 'parameters', 'pool_type', 'positional_embedding', 'register_backward_hook', 'register_buffer', 'register_forward_hook', 'register_forward_pre_hook', 'register_full_backward_hook', 'register_full_backward_pre_hook', 'register_load_state_dict_post_hook', 'register_load_state_dict_pre_hook', 'register_module', 'register_parameter', 'register_state_dict_post_hook', 'register_state_dict_pre_hook', 'requires_grad_', 'set_extra_state', 'set_submodule', 'share_memory', 'state_dict', 'text_global_pool', 'text_projection', 'to', 'to_empty', 'token_embedding', 'train', 'training', 'transformer', 'type', 'visual', 'vocab_size', 'width', 'xpu', 'zero_grad']\n",
      "224\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Module.add_module() missing 1 required positional argument: 'module'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m model.parameters():\n\u001b[32m      5\u001b[39m     param.requires_grad = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mclassifier\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: Module.add_module() missing 1 required positional argument: 'module'"
     ]
    }
   ],
   "source": [
    "print(dir(model))\n",
    "print(model.image_size)\n",
    "# Freeze all initially\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "\n",
    "model.add_module('classifier')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    start_frame  end_frame                      action\n",
      "0             0         63                     nothing\n",
      "1            63         70   checking rear view mirror\n",
      "2            70         85   checking left wing mirror\n",
      "3            85        128                     nothing\n",
      "4           129        144  checking right wing mirror\n",
      "..          ...        ...                         ...\n",
      "95         9965       9978   checking left wing mirror\n",
      "96         9979      10025   checking rear view mirror\n",
      "97        10026      10034                     nothing\n",
      "98        10035      10065   checking rear view mirror\n",
      "99        10066      10180                     nothing\n",
      "\n",
      "[100 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "data = np.load('/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/proper_cartesting/1/1.npy', allow_pickle=True).item()\n",
    "\n",
    "\n",
    "\n",
    "frames = data['rgb']\n",
    "frame_ts = list(frames.keys())\n",
    "frames = list(frames.values())\n",
    "import pandas as pd\n",
    "\n",
    "action_intervals = pd.read_csv('/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/notebooks_and_scripts/actions.csv')\n",
    "\n",
    "print(action_intervals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lwm = frames[7980:8010]\n",
    "\n",
    "\n",
    "for i, frame in enumerate(lwm):\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    cv2.imshow(\"Gaze Visualization\", frame_rgb)\n",
    "    key = cv2.waitKey(250)\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "output_path = \"/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rwm_4.mp4\"\n",
    "height, width, layers = lwm[0].shape\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "video_writer = cv2.VideoWriter(output_path, fourcc, 15, (width, height))\n",
    "\n",
    "for frame in lwm:\n",
    "    video_writer.write(frame)\n",
    "\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/nothing_5.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.08 0.07 0.74 0.11\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rvm_4.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.15 0.10 0.65 0.09\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rvm_5.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.12 0.09 0.70 0.09\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/nothing_4.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.09 0.08 0.69 0.14\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rvm_1.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.09 0.06 0.78 0.07\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/nothing_1.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.16 0.10 0.63 0.11\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/nothing_3.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.10 0.07 0.77 0.06\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rvm_2.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.11 0.08 0.70 0.11\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rvm_3.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.11 0.09 0.68 0.11\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/nothing_2.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.09 0.08 0.61 0.21\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rwm_3.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.10 0.07 0.58 0.26\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rwm_2.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.16 0.12 0.62 0.10\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/rwm_1.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.15 0.10 0.64 0.11\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/lwm_1.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.06 0.04 0.84 0.06\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/lwm_3.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.07 0.05 0.84 0.04\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/lwm_2.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.05 0.04 0.76 0.15\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/lwm_5.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.07 0.04 0.79 0.10\n",
      "This video is about checking rearview mirror\n",
      "/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/lwm_4.mp4\n",
      "Captions: ['checking left wing mirror ', 'checking right wing mirror', 'checking rearview mirror', 'driving']\n",
      "Label probs: 0.06 0.04 0.72 0.18\n",
      "This video is about checking rearview mirror\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "paths = os.listdir('/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/')\n",
    "for path in paths:\n",
    "    if path.endswith('.mp4'):\n",
    "        video, first_frame = preprocess_video(f'/Users/michaelrice/Documents/GitHub/Thesis/MSc_AI_Thesis/sampledata/vid_test/action_videos/{path}', 30, transform=preprocess)\n",
    "        video = video.unsqueeze(0).to(device)\n",
    "        text = tokenizer([\"checking left wing mirror \", \"checking right wing mirror\", 'checking rearview mirror', \"driving\"]).to(device)\n",
    "        captions = [\"checking left wing mirror \", \"checking right wing mirror\", 'checking rearview mirror', \"driving\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_video(video)\n",
    "            text_features = model.encode_text(text)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "            text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1).cpu().numpy()[0]\n",
    "\n",
    "\n",
    "        print(\"Captions:\", captions)\n",
    "        print(\"Label probs:\", ' '.join(['{:.2f}'.format(prob) for prob in text_probs]))  # prints: [[0.00, 1.00, 0.00]]\n",
    "        print(f\"This video is about {captions[text_probs.argmax()]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose model.encode_video outputs features of shape [B, D]\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class VideoActionClassifier(nn.Module):\n",
    "    def __init__(self, base_model, embed_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.classifier = nn.Linear(embed_dim, num_classes)\n",
    "    \n",
    "    def forward(self, video):\n",
    "        with torch.no_grad():\n",
    "            features = self.base_model.encode_video(video)  # freeze video encoder initially\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        logits = self.classifier(features)  # [B, num_classes]\n",
    "        return logits\n",
    "\n",
    "# Instantiate\n",
    "num_classes = 3\n",
    "num_epochs = 5\n",
    "embed_dim = 224  # adjust to your model's output dim\n",
    "classifier_model = VideoActionClassifier(model, embed_dim, num_classes).to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(classifier_model.classifier.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop (simplified)\n",
    "for epoch in range(num_epochs):\n",
    "    for video_batch, labels in train_loader:\n",
    "        video_batch = video_batch.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        logits = classifier_model(video_batch)\n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
